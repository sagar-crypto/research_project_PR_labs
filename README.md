# âš¡ PR-Labs Research Project â€” Time-Series Fault Detection

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.2+-ee4c2c?logo=pytorch)
![Transformers](https://img.shields.io/badge/Transformers-HuggingFace-yellow?logo=huggingface)
![License](https://img.shields.io/badge/license-MIT-green)
![Status](https://img.shields.io/badge/Status-Active-success)

---

## ğŸš€ Overview

This repository contains the complete deep learning pipeline for **time-series fault detection and anomaly analysis** developed at **PR-Labs (FAU Erlangen-NÃ¼rnberg)**.

The project combines:

* ğŸ§  **Custom Transformer Autoencoder (PredTrAD-inspired)**
* ğŸ¤– **Hugging Face TimeSeriesTransformer fine-tuning**
* ğŸ” **Variational Autoencoder (VAE) for latent representation**
* ğŸ“Š **Unsupervised clustering and evaluation**

---

## ğŸ—‚ Project Structure

```
research_project_PR_labs/
â”œâ”€â”€ transformer/                    # Custom Transformer Autoencoder + Classifier
â”‚   â”œâ”€â”€ train.py                    # Train transformer autoencoder
â”‚   â”œâ”€â”€ train_classification_head.py# Train classification head
â”‚   â”œâ”€â”€ model.py                    # Model definitions
â”‚   â”œâ”€â”€ data_processing.py          # Dataset and loader
â”‚   â”œâ”€â”€ utils.py                    # Helper functions
â”‚   â”œâ”€â”€ utils_label.py              # Helper functions to map the labels to data
â”‚   â”œâ”€â”€ clasify_config.json         # config for classification head
â”‚   â””â”€â”€ hyperParameters.json        # Transformer config
â”‚
â”œâ”€â”€ hugging_face_transformer/       # Hugging Face TimeSeriesTransformer head training
â”‚   â”œâ”€â”€ train.py                    # Train transformer autoencoder
â”‚   â”œâ”€â”€ utils.py                    # Helper functions
â”‚   â”œâ”€â”€ train_linear_head.py        # Train classification head
â”‚   â”œâ”€â”€ memmap_dataset.py           # Another data loader when a memmap dataset is passed
â”‚   â”œâ”€â”€ linear_head.py              # linear head model
â”‚   â”œâ”€â”€ data_processing.py          # Dataset and loader
â”‚   â”œâ”€â”€ classify_config.json        # config for classification head
â”‚   â””â”€â”€ hyper_parameter.json        # Transformer config
â”‚
â”œâ”€â”€ VAE/                            # Variational Autoencoder implementation
â”‚   â”œâ”€â”€ train.py                    # Train autoencoder
â”‚   â”œâ”€â”€ clustering_util.py          # Helper function for Clustering
â”‚   â”œâ”€â”€ model.py                    # Model definitions
â”‚   â”œâ”€â”€ ploting_util.py             # Helper function for ploting
â”‚   â”œâ”€â”€ 6_clusters.py               # Clustering Helper
â”‚   â”œâ”€â”€ data_processing.py          # Dataset and loader
â”‚   â””â”€â”€ hyper_parameter.json        # Train transformer autoencoder
â”‚
â”œâ”€â”€ external_libs/                  # External cloned dependencies
â”‚   â”œâ”€â”€ PredTrADv1/
â”‚   â””â”€â”€ PredTrADv2/
â”‚
â”œâ”€â”€ scripts/                        # Data and label processing scripts
â”œâ”€â”€ checkpoints/                    # Saved model checkpoints
â”œâ”€â”€ config.py                       # Global path configuration
â”œâ”€â”€ pyproject.toml                  # Package definition (pip installable)
â”œâ”€â”€ environment.lock.yml            # Fully reproducible Conda environment
â”œâ”€â”€ requirements.lock.txt           # Pinned pip requirements
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment Setup

### ğŸ§  Option 1 â€” Reproduce the Exact Working Environment

```bash
# Clone repository
git clone https://github.com/<your-username>/research_project_PR_labs.git
cd research_project_PR_labs

# Create & activate environment
mamba env create -f environment.lock.yml
mamba activate prlabs-ts

# Install correct PyTorch build for your hardware (choose one)
# CUDA 12.1
pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio
# or CPU-only
pip install --index-url https://download.pytorch.org/whl/cpu torch torchvision torchaudio

# Install project
pip install -e .
```

### ğŸ§¬ Option 2 â€” Lightweight Setup (Flexible Versions)

```bash
conda create -n prlabs-ts python=3.10
conda activate prlabs-ts

pip install -e .
pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio
```

---

## âš¡ Quick Start Commands

### ğŸ”¹ Train Transformer Autoencoder

```bash
python -m transformer.train
```

### ğŸ”¹ Train Linear Classification Head

```bash
python -m transformer.train_classification_head
```

### ğŸ”¹ Train Hugging Face Transformer

```bash
python -m hugging_face_transformer.train
```

### ğŸ”¹ Fine-Tune Hugging Face Transformer Head

```bash
python -m hugging_face_transformer.train_linear_head
```

### ğŸ”¹ Train PredTrad V1 Transformer

```bash
# paths
PREDTRAD_ROOT="$HOME/research_project_PR_labs/external_libs/PredTrAD"
CONFIG_PATH="$PREDTRAD_ROOT/config/epx4/myconfig.json"

# (optional) limit input for smoke tests (0 = no limit)
export MAX_FILES=0
export MAX_SAMPLES=0

# (optional) pick a GPU
# export CUDA_VISIBLE_DEVICES=0

# launch training (experiment4 entrypoint)
python "$PREDTRAD_ROOT/predtrad_impl.py" experiment4 \
  --config "$CONFIG_PATH"
```

### ğŸ”¹ Train Variational Autoencoder

```bash
python -m VAE.train
```

ğŸ— All training outputs (checkpoints, logs, and metrics) are saved under:

```
checkpoints/
mlruns/
```

---

## ğŸ“Š Pipeline Overview

| Stage                       | Description                                  | Script                                      |
| --------------------------- | -------------------------------------------- | ------------------------------------------- |
| **Data Preparation**        | Preprocess and label parquet/CSV sensor data | `scripts/make_clean_labels.py`              |
| **Transformer AE Training** | Unsupervised feature learning                | `transformer/train.py`                      |
| **Fault Head Training**     | Linear classification head                   | `transformer/train_classification_head.py`  |
| **Hugging Face Head**       | Linear head fine-tuning                      | `hugging_face_transformer/train_hf_head.py` |
| **VAE Training**            | Latent representation learning               | `VAE/train_vae.py`                          |
| **Clustering & Evaluation** | Latent space visualization & metrics         | `clustering_utils.py`                       |

---

## âš™ï¸ Key Configuration Files

| File                                            | Purpose                            |
| ----------------------------------------------- | ---------------------------------- |
| `transformer/hyperParameters.json`              | Transformer AE hyperparameters     |
| `hugging_face_transformer/hyper_parameter.json` | HF transformer configuration       |
| `config.py`                                     | Global paths and directories       |
| `pyproject.toml`                                | Project metadata (for pip install) |
| `environment.lock.yml`                          | Exact dependency snapshot          |

---

## ğŸ“ˆ Outputs & Artifacts

| Artifact               | Description                       | Location                        |
| ---------------------- | --------------------------------- | ------------------------------- |
| ğŸ¤© **Model Weights**   | Trained checkpoints (.pth)        | `checkpoints/`                  |
| ğŸ”¢ **Latent Features** | Encoded representations (.npy)    | `latent_features.npy`           |
| ğŸ“Š **Metrics JSON**    | Precision, Recall, F1, AUC        | `fault_classifier_metrics.json` |
| ğŸ“‰ **Loss Curves**     | Training & validation loss        | `loss_validation_plot.png`      |
| ğŸ” **Reconstructions** | Original vs reconstructed signals | `clustering_img/`               |

---

## ğŸ§  Reproducibility Tips

* Ensure identical preprocessing during training and inference
* Match normalization bounds (`feature_min`, `feature_max`)
* Verify checkpoint paths in `config.py`
* Use `environment.lock.yml` for exact dependencies

---

## ğŸŒ External Dependencies

This project integrates:

* [Hugging Face Transformers](https://github.com/huggingface/transformers)
* [PredTrAD (Predictive Transformer for Anomaly Detection)](https://github.com/your-external-lib)
* [MLflow](https://mlflow.org/) for experiment tracking

---

## ğŸ¤ Contribution

Contributions are welcome!
Please open an issue for bugs, ideas, or feature requests before submitting a pull request.

---

## ğŸ“ Contact

**Author:** Sagar Sikdar
ğŸ“§ Email: [sagar.sikdar@fau.de](mailto:sagar.sikdar@fau.de)
ğŸŒ GitHub: [github.com/sagar-crypto](https://github.com/sagar-crypto)

---

## ğŸ”® Future Improvements

* ğŸ§± Unified CLI (`train --model transformer/vae/hf`)
* ğŸ³ Docker image for full reproducibility
* ğŸ“Š Visualization dashboards for clustering metrics
* âš™ï¸ Add Lightning or Hydra config management

---

## ğŸ““ Table of Contents

* [Overview](#-overview)
* [Project Structure](#-project-structure)
* [Environment Setup](#%EF%B8%8F-environment-setup)
* [Quick Start](#-quick-start-commands)
* [Pipeline Overview](#-pipeline-overview)
* [Key Configuration Files](#-key-configuration-files)
* [Outputs](#-outputs--artifacts)
* [Reproducibility Tips](#-reproducibility-tips)
* [External Dependencies](#-external-dependencies)
* [Contribution](#-contribution)
* [Contact](#-contact)
* [Future Improvements](#-future-improvements)

---

### ğŸŒŸ TL;DR â€” 1-Line Setup

```bash
mamba env create -f environment.lock.yml && mamba activate prlabs-ts && pip install -e .
```

ğŸš€ *Youâ€™re ready to train, cluster, and detect faults!*
